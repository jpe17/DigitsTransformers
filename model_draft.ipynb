{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders import MnistLoader, SquareImageSplitingLoader\n",
    "\n",
    "mnist_loader = MnistLoader()\n",
    "\n",
    "train_loader, validation_loader = mnist_loader.get_loaders()\n",
    "\n",
    "square_image_spliting_loader = SquareImageSplitingLoader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SimpleVisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Vision Transformer for MNIST classification\n",
    "    Focus: Understanding the basics step by step\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_dim=49, embed_dim=32, num_patches=16, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Step 1: Patch Embedding\n",
    "        self.patch_embedding = nn.Linear(patch_dim, embed_dim)\n",
    "        print(f\"üì¶ Patch embedding: {patch_dim} -> {embed_dim}\")\n",
    "        \n",
    "        # Step 2: Positional Encoding (learnable - simpler than sinusoidal)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        print(f\"üìç Positional embedding: {self.pos_embedding.shape}\")\n",
    "        \n",
    "        # Step 3: Self-Attention (single head)\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        print(f\"üîç Self-attention matrices: 3 x ({embed_dim} -> {embed_dim})\")\n",
    "        \n",
    "        # Step 4: Feed Forward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),  # Expand\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),  # Contract\n",
    "        )\n",
    "        print(f\"üîÑ FFN: {embed_dim} -> {embed_dim * 2} -> {embed_dim}\")\n",
    "        \n",
    "        # Step 5: Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Step 6: Classification Head\n",
    "        # We'll use global average pooling across patches\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "        print(f\"üéØ Classifier: {embed_dim} -> {num_classes}\")\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with detailed comments for learning\n",
    "        Input: x shape [batch_size, 16, 1, 7, 7] - 16 patches per image\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Step 1: Flatten patches and embed them\n",
    "        x = x.flatten(start_dim=2)  # [batch_size, 16, 49]\n",
    "        x = self.patch_embedding(x)  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 2: Add positional encoding\n",
    "        x = x + self.pos_embedding  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 3: Self-Attention Block\n",
    "        # Save input for residual connection\n",
    "        residual = x\n",
    "        \n",
    "        # Apply layer norm BEFORE attention (Pre-LN architecture)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.W_q(x)  # [batch_size, 16, 32]\n",
    "        K = self.W_k(x)  # [batch_size, 16, 32] \n",
    "        V = self.W_v(x)  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_scores = Q @ K.transpose(-2, -1) / (self.embed_dim ** 0.5)  # [batch_size, 16, 16]\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attended_values = attention_weights @ V  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Add residual connection\n",
    "        x = residual + attended_values  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 4: Feed Forward Block\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + x  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 5: Classification\n",
    "        # Global average pooling across patches\n",
    "        x = x.mean(dim=1)  # [batch_size, 32] - average across all patches\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(x)  # [batch_size, 10]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create the model\n",
    "print(\"=== CREATING SIMPLE VISION TRANSFORMER ===\")\n",
    "model = SimpleVisionTransformer()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\nüß™ Testing forward pass...\")\n",
    "for patches, labels in square_image_spliting_loader:\n",
    "    print(f\"Input shape: {patches.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    logits = model(patches)\n",
    "    print(f\"Output logits: {logits.shape}\")\n",
    "    print(f\"Sample predictions: {logits[0]}\")\n",
    "    \n",
    "    break\n",
    "\n",
    "print(\"\\n‚úÖ Model ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    Simple training loop with detailed explanations\n",
    "    Focus: Understanding each step of the training process\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f\"=== TRAINING SETUP ===\")\n",
    "    print(f\"üìö Epochs: {num_epochs}\")\n",
    "    print(f\"üìà Learning rate: {learning_rate}\")\n",
    "    print(f\"üí° Optimizer: Adam\")\n",
    "    print(f\"üéØ Loss function: CrossEntropyLoss\")\n",
    "    \n",
    "    # Track training progress\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    print(f\"\\n=== STARTING TRAINING ===\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # === TRAINING PHASE ===\n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        num_batches = 0  # Track number of batches manually\n",
    "        \n",
    "        print(f\"\\nüîÑ Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"   Training...\")\n",
    "        \n",
    "        for batch_idx, (patches, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            logits = model(patches)  # [batch_size, 10]\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            loss.backward()        # Compute gradients\n",
    "            optimizer.step()       # Update parameters\n",
    "            \n",
    "            # Track statistics\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                current_acc = 100. * correct_predictions / total_samples\n",
    "                print(f\"     Batch {batch_idx:3d}: Loss = {loss.item():.4f}, Acc = {current_acc:.1f}%\")\n",
    "        \n",
    "        # Calculate epoch statistics using batch count instead of len()\n",
    "        avg_train_loss = total_train_loss / num_batches\n",
    "        train_accuracy = 100. * correct_predictions / total_samples\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # === VALIDATION PHASE ===\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        print(\"   Validating...\")\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            for patches, labels in val_loader:\n",
    "                logits = model(patches)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"   üìä Results:\")\n",
    "        print(f\"      Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"      Train Acc:  {train_accuracy:.2f}%\")\n",
    "        print(f\"      Val Acc:    {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Simple early stopping if validation accuracy is very high\n",
    "        if val_accuracy > 95.0:\n",
    "            print(f\"   üéâ Great accuracy achieved! Stopping early.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n=== TRAINING COMPLETED ===\")\n",
    "    print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "\n",
    "# First, let's create proper train and validation loaders\n",
    "train_patch_loader = SquareImageSplitingLoader(train_loader)\n",
    "val_patch_loader = SquareImageSplitingLoader(validation_loader)\n",
    "\n",
    "print(\"=== DATA SETUP ===\")\n",
    "print(f\"üìö Training data ready\")\n",
    "print(f\"üìñ Validation data ready\")\n",
    "\n",
    "# Let's train our model!\n",
    "print(\"\\nStarting training process...\")\n",
    "history = train_model(model, train_patch_loader, val_patch_loader, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's fix the validation loader issue and create a proper validation set\n",
    "from loaders import MnistLoader, SquareImageSplitingLoader\n",
    "\n",
    "# Reload with proper validation\n",
    "mnist_loader = MnistLoader()\n",
    "train_loader, validation_loader = mnist_loader.get_loaders()\n",
    "\n",
    "# Create patch loaders for both train and validation\n",
    "train_patch_loader = SquareImageSplitingLoader(train_loader)\n",
    "val_patch_loader = SquareImageSplitingLoader(validation_loader)\n",
    "\n",
    "print(\"=== DATA SETUP ===\")\n",
    "print(f\"üìö Training batches: {len(train_loader)}\")\n",
    "print(f\"üìñ Validation batches: {len(validation_loader)}\")\n",
    "\n",
    "# Now let's train with proper validation\n",
    "print(\"=== RETRAINING WITH PROPER VALIDATION ===\")\n",
    "model_v2 = SimpleVisionTransformer()  # Fresh model\n",
    "history = train_model(model_v2, train_patch_loader, val_patch_loader, num_epochs=100, learning_rate=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_model_understanding(model, val_loader):\n",
    "    \"\"\"\n",
    "    Analyze what the model learned - for educational purposes\n",
    "    \"\"\"\n",
    "    print(\"=== MODEL ANALYSIS ===\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Test on a few examples\n",
    "    correct_predictions = []\n",
    "    wrong_predictions = []\n",
    "    attention_patterns = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (patches, labels) in enumerate(val_loader):\n",
    "            if batch_idx > 5:  # Just analyze first few batches\n",
    "                break\n",
    "                \n",
    "            # Get model predictions\n",
    "            logits = model(patches)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            \n",
    "            # Store examples\n",
    "            for i in range(min(5, patches.size(0))):  # First 5 images in batch\n",
    "                actual = labels[i].item()\n",
    "                pred = predicted[i].item()\n",
    "                \n",
    "                if actual == pred:\n",
    "                    correct_predictions.append((patches[i], actual, pred))\n",
    "                else:\n",
    "                    wrong_predictions.append((patches[i], actual, pred))\n",
    "                \n",
    "                if len(correct_predictions) >= 5 and len(wrong_predictions) >= 3:\n",
    "                    break\n",
    "            \n",
    "            if len(correct_predictions) >= 5 and len(wrong_predictions) >= 3:\n",
    "                break\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"‚úÖ Found {len(correct_predictions)} correct predictions\")\n",
    "    print(f\"‚ùå Found {len(wrong_predictions)} wrong predictions\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\nüìä CORRECT PREDICTIONS:\")\n",
    "    for i, (patches, actual, pred) in enumerate(correct_predictions[:3]):\n",
    "        print(f\"   Example {i+1}: Actual = {actual}, Predicted = {pred} ‚úì\")\n",
    "    \n",
    "    print(\"\\nüìä WRONG PREDICTIONS:\")\n",
    "    for i, (patches, actual, pred) in enumerate(wrong_predictions[:3]):\n",
    "        print(f\"   Example {i+1}: Actual = {actual}, Predicted = {pred} ‚úó\")\n",
    "    \n",
    "    return correct_predictions, wrong_predictions\n",
    "\n",
    "def visualize_training_progress(history):\n",
    "    \"\"\"\n",
    "    Simple visualization of training progress\n",
    "    \"\"\"\n",
    "    print(\"\\n=== TRAINING PROGRESS ===\")\n",
    "    \n",
    "    epochs = range(1, len(history['train_losses']) + 1)\n",
    "    \n",
    "    print(\"üìà Training Progress:\")\n",
    "    for i, epoch in enumerate(epochs):\n",
    "        print(f\"   Epoch {epoch}:\")\n",
    "        print(f\"      Loss: {history['train_losses'][i]:.4f}\")\n",
    "        print(f\"      Train Acc: {history['train_accuracies'][i]:.2f}%\")\n",
    "        print(f\"      Val Acc: {history['val_accuracies'][i]:.2f}%\")\n",
    "    \n",
    "    # Show improvement\n",
    "    initial_val_acc = history['val_accuracies'][0]\n",
    "    final_val_acc = history['val_accuracies'][-1]\n",
    "    improvement = final_val_acc - initial_val_acc\n",
    "    \n",
    "    print(f\"\\nüéØ OVERALL IMPROVEMENT:\")\n",
    "    print(f\"   Initial validation accuracy: {initial_val_acc:.2f}%\")\n",
    "    print(f\"   Final validation accuracy: {final_val_acc:.2f}%\")\n",
    "    print(f\"   Improvement: +{improvement:.2f}%\")\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "# Analyze our trained model\n",
    "correct_examples, wrong_examples = analyze_model_understanding(model_v2, val_patch_loader)\n",
    "epochs = visualize_training_progress(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
