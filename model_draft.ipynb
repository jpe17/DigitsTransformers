{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist dataset loaded, train data size: 50000 validation data size: 10000\n"
     ]
    }
   ],
   "source": [
    "from loaders import MnistLoader, SquareImageSplitingLoader\n",
    "\n",
    "mnist_loader = MnistLoader()\n",
    "\n",
    "train_loader, validation_loader = mnist_loader.get_loaders()\n",
    "\n",
    "square_image_spliting_loader = SquareImageSplitingLoader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING SIMPLE VISION TRANSFORMER ===\n",
      "📦 Patch embedding: 49 -> 32\n",
      "📍 Positional embedding: torch.Size([1, 16, 32])\n",
      "🔍 Self-attention matrices: 3 x (32 -> 32)\n",
      "🔄 FFN: 32 -> 64 -> 32\n",
      "🎯 Classifier: 32 -> 10\n",
      "\n",
      "📊 Model Statistics:\n",
      "   Total parameters: 9,930\n",
      "   Trainable parameters: 9,930\n",
      "\n",
      "🧪 Testing forward pass...\n",
      "Input shape: torch.Size([128, 16, 1, 7, 7])\n",
      "Labels shape: torch.Size([128])\n",
      "Output logits: torch.Size([128, 10])\n",
      "Sample predictions: tensor([-0.0328, -0.0004,  0.0249,  0.2919,  0.0047,  0.0679,  0.0348, -0.0929,\n",
      "        -0.1317, -0.0372], grad_fn=<SelectBackward0>)\n",
      "\n",
      "✅ Model ready for training!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SimpleVisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Vision Transformer for MNIST classification\n",
    "    Focus: Understanding the basics step by step\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_dim=49, embed_dim=32, num_patches=16, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Step 1: Patch Embedding\n",
    "        self.patch_embedding = nn.Linear(patch_dim, embed_dim)\n",
    "        print(f\"📦 Patch embedding: {patch_dim} -> {embed_dim}\")\n",
    "        \n",
    "        # Step 2: Positional Encoding (learnable - simpler than sinusoidal)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        print(f\"📍 Positional embedding: {self.pos_embedding.shape}\")\n",
    "        \n",
    "        # Step 3: Self-Attention (single head)\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        print(f\"🔍 Self-attention matrices: 3 x ({embed_dim} -> {embed_dim})\")\n",
    "        \n",
    "        # Step 4: Feed Forward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),  # Expand\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),  # Contract\n",
    "        )\n",
    "        print(f\"🔄 FFN: {embed_dim} -> {embed_dim * 2} -> {embed_dim}\")\n",
    "        \n",
    "        # Step 5: Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Step 6: Classification Head\n",
    "        # We'll use global average pooling across patches\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "        print(f\"🎯 Classifier: {embed_dim} -> {num_classes}\")\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with detailed comments for learning\n",
    "        Input: x shape [batch_size, 16, 1, 7, 7] - 16 patches per image\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Step 1: Flatten patches and embed them\n",
    "        x = x.flatten(start_dim=2)  # [batch_size, 16, 49]\n",
    "        x = self.patch_embedding(x)  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 2: Add positional encoding\n",
    "        x = x + self.pos_embedding  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 3: Self-Attention Block\n",
    "        # Save input for residual connection\n",
    "        residual = x\n",
    "        \n",
    "        # Apply layer norm BEFORE attention (Pre-LN architecture)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.W_q(x)  # [batch_size, 16, 32]\n",
    "        K = self.W_k(x)  # [batch_size, 16, 32] \n",
    "        V = self.W_v(x)  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_scores = Q @ K.transpose(-2, -1) / (self.embed_dim ** 0.5)  # [batch_size, 16, 16]\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attended_values = attention_weights @ V  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Add residual connection\n",
    "        x = residual + attended_values  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 4: Feed Forward Block\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + x  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 5: Classification\n",
    "        # Global average pooling across patches\n",
    "        x = x.mean(dim=1)  # [batch_size, 32] - average across all patches\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(x)  # [batch_size, 10]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create the model\n",
    "print(\"=== CREATING SIMPLE VISION TRANSFORMER ===\")\n",
    "model = SimpleVisionTransformer()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n📊 Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\n🧪 Testing forward pass...\")\n",
    "for patches, labels in square_image_spliting_loader:\n",
    "    print(f\"Input shape: {patches.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    logits = model(patches)\n",
    "    print(f\"Output logits: {logits.shape}\")\n",
    "    print(f\"Sample predictions: {logits[0]}\")\n",
    "    \n",
    "    break\n",
    "\n",
    "print(\"\\n✅ Model ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SETUP ===\n",
      "📚 Training data ready\n",
      "📖 Validation data ready\n",
      "\n",
      "Starting training process...\n",
      "=== TRAINING SETUP ===\n",
      "📚 Epochs: 100\n",
      "📈 Learning rate: 0.0001\n",
      "💡 Optimizer: Adam\n",
      "🎯 Loss function: CrossEntropyLoss\n",
      "\n",
      "=== STARTING TRAINING ===\n",
      "\n",
      "🔄 Epoch 1/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 2.3093, Acc = 11.7%\n",
      "     Batch  50: Loss = 2.3009, Acc = 9.8%\n",
      "     Batch 100: Loss = 2.2847, Acc = 11.8%\n",
      "     Batch 150: Loss = 2.2895, Acc = 15.4%\n",
      "     Batch 200: Loss = 2.2751, Acc = 17.8%\n",
      "     Batch 250: Loss = 2.2725, Acc = 18.9%\n",
      "     Batch 300: Loss = 2.2428, Acc = 20.3%\n",
      "     Batch 350: Loss = 2.2136, Acc = 21.6%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 2.2656\n",
      "      Train Acc:  22.42%\n",
      "      Val Acc:    30.05%\n",
      "\n",
      "🔄 Epoch 2/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 2.1893, Acc = 31.2%\n",
      "     Batch  50: Loss = 2.1067, Acc = 30.8%\n",
      "     Batch 100: Loss = 2.0465, Acc = 30.4%\n",
      "     Batch 150: Loss = 1.8809, Acc = 30.9%\n",
      "     Batch 200: Loss = 1.8490, Acc = 31.7%\n",
      "     Batch 250: Loss = 1.7871, Acc = 32.8%\n",
      "     Batch 300: Loss = 1.7684, Acc = 34.0%\n",
      "     Batch 350: Loss = 1.7185, Acc = 35.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 1.8627\n",
      "      Train Acc:  36.59%\n",
      "      Val Acc:    47.20%\n",
      "\n",
      "🔄 Epoch 3/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 1.5372, Acc = 51.6%\n",
      "     Batch  50: Loss = 1.6491, Acc = 47.7%\n",
      "     Batch 100: Loss = 1.5049, Acc = 49.8%\n",
      "     Batch 150: Loss = 1.5471, Acc = 50.7%\n",
      "     Batch 200: Loss = 1.5229, Acc = 51.5%\n",
      "     Batch 250: Loss = 1.5076, Acc = 52.1%\n",
      "     Batch 300: Loss = 1.3543, Acc = 53.1%\n",
      "     Batch 350: Loss = 1.3723, Acc = 53.8%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 1.4548\n",
      "      Train Acc:  54.46%\n",
      "      Val Acc:    60.10%\n",
      "\n",
      "🔄 Epoch 4/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 1.3441, Acc = 58.6%\n",
      "     Batch  50: Loss = 1.2683, Acc = 59.9%\n",
      "     Batch 100: Loss = 1.2693, Acc = 60.6%\n",
      "     Batch 150: Loss = 1.2118, Acc = 61.0%\n",
      "     Batch 200: Loss = 1.3252, Acc = 61.4%\n",
      "     Batch 250: Loss = 1.0558, Acc = 61.5%\n",
      "     Batch 300: Loss = 1.0328, Acc = 62.0%\n",
      "     Batch 350: Loss = 1.1156, Acc = 62.2%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 1.2338\n",
      "      Train Acc:  62.43%\n",
      "      Val Acc:    65.49%\n",
      "\n",
      "🔄 Epoch 5/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 1.0199, Acc = 68.0%\n",
      "     Batch  50: Loss = 1.0406, Acc = 65.9%\n",
      "     Batch 100: Loss = 1.0436, Acc = 66.0%\n",
      "     Batch 150: Loss = 1.1320, Acc = 66.0%\n",
      "     Batch 200: Loss = 1.0879, Acc = 66.3%\n",
      "     Batch 250: Loss = 1.1151, Acc = 66.6%\n",
      "     Batch 300: Loss = 1.0439, Acc = 66.8%\n",
      "     Batch 350: Loss = 0.9388, Acc = 67.0%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 1.0535\n",
      "      Train Acc:  67.29%\n",
      "      Val Acc:    70.00%\n",
      "\n",
      "🔄 Epoch 6/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 1.0486, Acc = 69.5%\n",
      "     Batch  50: Loss = 0.8981, Acc = 69.3%\n",
      "     Batch 100: Loss = 1.0967, Acc = 69.8%\n",
      "     Batch 150: Loss = 0.8995, Acc = 70.1%\n",
      "     Batch 200: Loss = 0.9017, Acc = 70.6%\n",
      "     Batch 250: Loss = 0.7908, Acc = 71.0%\n",
      "     Batch 300: Loss = 0.8088, Acc = 71.3%\n",
      "     Batch 350: Loss = 0.7395, Acc = 71.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.9139\n",
      "      Train Acc:  71.74%\n",
      "      Val Acc:    73.53%\n",
      "\n",
      "🔄 Epoch 7/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.9322, Acc = 71.1%\n",
      "     Batch  50: Loss = 0.8868, Acc = 73.3%\n",
      "     Batch 100: Loss = 0.7946, Acc = 74.0%\n",
      "     Batch 150: Loss = 0.8351, Acc = 73.9%\n",
      "     Batch 200: Loss = 0.7084, Acc = 74.3%\n",
      "     Batch 250: Loss = 0.7934, Acc = 74.5%\n",
      "     Batch 300: Loss = 0.7474, Acc = 74.7%\n",
      "     Batch 350: Loss = 0.8145, Acc = 74.9%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.8018\n",
      "      Train Acc:  75.19%\n",
      "      Val Acc:    77.07%\n",
      "\n",
      "🔄 Epoch 8/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.9445, Acc = 70.3%\n",
      "     Batch  50: Loss = 0.8566, Acc = 77.2%\n",
      "     Batch 100: Loss = 0.7794, Acc = 77.7%\n",
      "     Batch 150: Loss = 0.6361, Acc = 77.9%\n",
      "     Batch 200: Loss = 0.7869, Acc = 77.7%\n",
      "     Batch 250: Loss = 0.8310, Acc = 77.6%\n",
      "     Batch 300: Loss = 0.6722, Acc = 77.8%\n",
      "     Batch 350: Loss = 0.6198, Acc = 77.9%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.7145\n",
      "      Train Acc:  77.98%\n",
      "      Val Acc:    79.59%\n",
      "\n",
      "🔄 Epoch 9/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.7666, Acc = 81.2%\n",
      "     Batch  50: Loss = 0.5509, Acc = 79.1%\n",
      "     Batch 100: Loss = 0.7575, Acc = 78.9%\n",
      "     Batch 150: Loss = 0.5556, Acc = 79.0%\n",
      "     Batch 200: Loss = 0.6156, Acc = 79.4%\n",
      "     Batch 250: Loss = 0.6739, Acc = 79.8%\n",
      "     Batch 300: Loss = 0.5846, Acc = 79.9%\n",
      "     Batch 350: Loss = 0.6488, Acc = 80.0%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.6509\n",
      "      Train Acc:  80.06%\n",
      "      Val Acc:    81.36%\n",
      "\n",
      "🔄 Epoch 10/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.6795, Acc = 82.0%\n",
      "     Batch  50: Loss = 0.6242, Acc = 81.0%\n",
      "     Batch 100: Loss = 0.5592, Acc = 80.9%\n",
      "     Batch 150: Loss = 0.4771, Acc = 81.1%\n",
      "     Batch 200: Loss = 0.5771, Acc = 81.1%\n",
      "     Batch 250: Loss = 0.4673, Acc = 81.3%\n",
      "     Batch 300: Loss = 0.6283, Acc = 81.5%\n",
      "     Batch 350: Loss = 0.5977, Acc = 81.6%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.6051\n",
      "      Train Acc:  81.60%\n",
      "      Val Acc:    82.48%\n",
      "\n",
      "🔄 Epoch 11/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.5030, Acc = 86.7%\n",
      "     Batch  50: Loss = 0.6625, Acc = 81.9%\n",
      "     Batch 100: Loss = 0.4875, Acc = 81.8%\n",
      "     Batch 150: Loss = 0.4754, Acc = 82.1%\n",
      "     Batch 200: Loss = 0.5279, Acc = 82.4%\n",
      "     Batch 250: Loss = 0.6888, Acc = 82.5%\n",
      "     Batch 300: Loss = 0.5409, Acc = 82.6%\n",
      "     Batch 350: Loss = 0.3895, Acc = 82.7%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.5702\n",
      "      Train Acc:  82.73%\n",
      "      Val Acc:    83.83%\n",
      "\n",
      "🔄 Epoch 12/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.5124, Acc = 88.3%\n",
      "     Batch  50: Loss = 0.5197, Acc = 83.1%\n",
      "     Batch 100: Loss = 0.5946, Acc = 83.1%\n",
      "     Batch 150: Loss = 0.5630, Acc = 83.3%\n",
      "     Batch 200: Loss = 0.5736, Acc = 83.5%\n",
      "     Batch 250: Loss = 0.4516, Acc = 83.4%\n",
      "     Batch 300: Loss = 0.4542, Acc = 83.5%\n",
      "     Batch 350: Loss = 0.4276, Acc = 83.6%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.5417\n",
      "      Train Acc:  83.65%\n",
      "      Val Acc:    84.35%\n",
      "\n",
      "🔄 Epoch 13/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.5835, Acc = 84.4%\n",
      "     Batch  50: Loss = 0.4485, Acc = 84.7%\n",
      "     Batch 100: Loss = 0.5355, Acc = 84.3%\n",
      "     Batch 150: Loss = 0.4283, Acc = 84.2%\n",
      "     Batch 200: Loss = 0.5329, Acc = 84.3%\n",
      "     Batch 250: Loss = 0.4961, Acc = 84.4%\n",
      "     Batch 300: Loss = 0.5113, Acc = 84.4%\n",
      "     Batch 350: Loss = 0.4437, Acc = 84.4%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.5187\n",
      "      Train Acc:  84.42%\n",
      "      Val Acc:    85.26%\n",
      "\n",
      "🔄 Epoch 14/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4211, Acc = 85.9%\n",
      "     Batch  50: Loss = 0.6087, Acc = 84.3%\n",
      "     Batch 100: Loss = 0.4178, Acc = 84.7%\n",
      "     Batch 150: Loss = 0.5194, Acc = 84.9%\n",
      "     Batch 200: Loss = 0.3665, Acc = 84.8%\n",
      "     Batch 250: Loss = 0.3445, Acc = 84.9%\n",
      "     Batch 300: Loss = 0.4750, Acc = 84.9%\n",
      "     Batch 350: Loss = 0.5399, Acc = 84.9%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.5001\n",
      "      Train Acc:  85.02%\n",
      "      Val Acc:    85.58%\n",
      "\n",
      "🔄 Epoch 15/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.5861, Acc = 81.2%\n",
      "     Batch  50: Loss = 0.4044, Acc = 85.2%\n",
      "     Batch 100: Loss = 0.4029, Acc = 85.0%\n",
      "     Batch 150: Loss = 0.4802, Acc = 85.4%\n",
      "     Batch 200: Loss = 0.4585, Acc = 85.2%\n",
      "     Batch 250: Loss = 0.5714, Acc = 85.2%\n",
      "     Batch 300: Loss = 0.4008, Acc = 85.3%\n",
      "     Batch 350: Loss = 0.4228, Acc = 85.4%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.4827\n",
      "      Train Acc:  85.46%\n",
      "      Val Acc:    85.88%\n",
      "\n",
      "🔄 Epoch 16/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4512, Acc = 86.7%\n",
      "     Batch  50: Loss = 0.4926, Acc = 85.2%\n",
      "     Batch 100: Loss = 0.5649, Acc = 85.7%\n",
      "     Batch 150: Loss = 0.4069, Acc = 85.8%\n",
      "     Batch 200: Loss = 0.4424, Acc = 85.8%\n",
      "     Batch 250: Loss = 0.4748, Acc = 85.8%\n",
      "     Batch 300: Loss = 0.4902, Acc = 85.9%\n",
      "     Batch 350: Loss = 0.5328, Acc = 85.9%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.4685\n",
      "      Train Acc:  85.98%\n",
      "      Val Acc:    86.30%\n",
      "\n",
      "🔄 Epoch 17/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2828, Acc = 90.6%\n",
      "     Batch  50: Loss = 0.4315, Acc = 86.0%\n",
      "     Batch 100: Loss = 0.3892, Acc = 86.2%\n",
      "     Batch 150: Loss = 0.3806, Acc = 86.4%\n",
      "     Batch 200: Loss = 0.4091, Acc = 86.3%\n",
      "     Batch 250: Loss = 0.4533, Acc = 86.3%\n",
      "     Batch 300: Loss = 0.4199, Acc = 86.4%\n",
      "     Batch 350: Loss = 0.4859, Acc = 86.4%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.4555\n",
      "      Train Acc:  86.37%\n",
      "      Val Acc:    86.56%\n",
      "\n",
      "🔄 Epoch 18/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4261, Acc = 89.1%\n",
      "     Batch  50: Loss = 0.6360, Acc = 86.0%\n",
      "     Batch 100: Loss = 0.4085, Acc = 86.3%\n",
      "     Batch 150: Loss = 0.5152, Acc = 86.4%\n",
      "     Batch 200: Loss = 0.4152, Acc = 86.6%\n",
      "     Batch 250: Loss = 0.4402, Acc = 86.6%\n",
      "     Batch 300: Loss = 0.5388, Acc = 86.6%\n",
      "     Batch 350: Loss = 0.4677, Acc = 86.7%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.4446\n",
      "      Train Acc:  86.65%\n",
      "      Val Acc:    86.83%\n",
      "\n",
      "🔄 Epoch 19/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4537, Acc = 84.4%\n",
      "     Batch  50: Loss = 0.3560, Acc = 87.5%\n",
      "     Batch 100: Loss = 0.4516, Acc = 87.1%\n",
      "     Batch 150: Loss = 0.4568, Acc = 87.0%\n",
      "     Batch 200: Loss = 0.3312, Acc = 86.9%\n",
      "     Batch 250: Loss = 0.4721, Acc = 87.1%\n",
      "     Batch 300: Loss = 0.3887, Acc = 87.1%\n",
      "     Batch 350: Loss = 0.2731, Acc = 87.1%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.4337\n",
      "      Train Acc:  87.15%\n",
      "      Val Acc:    87.10%\n",
      "\n",
      "🔄 Epoch 20/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.5901, Acc = 84.4%\n",
      "     Batch  50: Loss = 0.5097, Acc = 86.6%\n",
      "     Batch 100: Loss = 0.3689, Acc = 87.2%\n",
      "     Batch 150: Loss = 0.4452, Acc = 87.1%\n",
      "     Batch 200: Loss = 0.3778, Acc = 87.2%\n",
      "     Batch 250: Loss = 0.3699, Acc = 87.1%\n",
      "     Batch 300: Loss = 0.3685, Acc = 87.1%\n",
      "     Batch 350: Loss = 0.3643, Acc = 87.2%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.4247\n",
      "      Train Acc:  87.28%\n",
      "      Val Acc:    87.49%\n",
      "\n",
      "🔄 Epoch 21/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4632, Acc = 89.1%\n",
      "     Batch  50: Loss = 0.5501, Acc = 87.3%\n",
      "     Batch 100: Loss = 0.5407, Acc = 87.4%\n",
      "     Batch 150: Loss = 0.3050, Acc = 87.5%\n",
      "     Batch 200: Loss = 0.6770, Acc = 87.4%\n",
      "     Batch 250: Loss = 0.3915, Acc = 87.5%\n",
      "     Batch 300: Loss = 0.5657, Acc = 87.5%\n",
      "     Batch 350: Loss = 0.4457, Acc = 87.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.4156\n",
      "      Train Acc:  87.57%\n",
      "      Val Acc:    87.69%\n",
      "\n",
      "🔄 Epoch 22/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4219, Acc = 89.8%\n",
      "     Batch  50: Loss = 0.3221, Acc = 87.9%\n",
      "     Batch 100: Loss = 0.4155, Acc = 88.0%\n",
      "     Batch 150: Loss = 0.2038, Acc = 88.0%\n",
      "     Batch 200: Loss = 0.3501, Acc = 87.9%\n",
      "     Batch 250: Loss = 0.4976, Acc = 87.9%\n",
      "     Batch 300: Loss = 0.3712, Acc = 87.9%\n",
      "     Batch 350: Loss = 0.4163, Acc = 87.9%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.4077\n",
      "      Train Acc:  87.90%\n",
      "      Val Acc:    87.93%\n",
      "\n",
      "🔄 Epoch 23/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4605, Acc = 87.5%\n",
      "     Batch  50: Loss = 0.3571, Acc = 87.4%\n",
      "     Batch 100: Loss = 0.4101, Acc = 87.9%\n",
      "     Batch 150: Loss = 0.3588, Acc = 88.0%\n",
      "     Batch 200: Loss = 0.4208, Acc = 88.0%\n",
      "     Batch 250: Loss = 0.5042, Acc = 88.1%\n",
      "     Batch 300: Loss = 0.4286, Acc = 88.1%\n",
      "     Batch 350: Loss = 0.2839, Acc = 88.1%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3997\n",
      "      Train Acc:  88.07%\n",
      "      Val Acc:    88.29%\n",
      "\n",
      "🔄 Epoch 24/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4293, Acc = 88.3%\n",
      "     Batch  50: Loss = 0.4863, Acc = 88.1%\n",
      "     Batch 100: Loss = 0.4071, Acc = 88.4%\n",
      "     Batch 150: Loss = 0.5078, Acc = 88.3%\n",
      "     Batch 200: Loss = 0.4050, Acc = 88.3%\n",
      "     Batch 250: Loss = 0.5552, Acc = 88.3%\n",
      "     Batch 300: Loss = 0.2693, Acc = 88.3%\n",
      "     Batch 350: Loss = 0.4953, Acc = 88.3%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3925\n",
      "      Train Acc:  88.32%\n",
      "      Val Acc:    88.42%\n",
      "\n",
      "🔄 Epoch 25/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4137, Acc = 88.3%\n",
      "     Batch  50: Loss = 0.4493, Acc = 88.5%\n",
      "     Batch 100: Loss = 0.4088, Acc = 88.6%\n",
      "     Batch 150: Loss = 0.3381, Acc = 88.7%\n",
      "     Batch 200: Loss = 0.4631, Acc = 88.6%\n",
      "     Batch 250: Loss = 0.2544, Acc = 88.5%\n",
      "     Batch 300: Loss = 0.4671, Acc = 88.4%\n",
      "     Batch 350: Loss = 0.3987, Acc = 88.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3853\n",
      "      Train Acc:  88.56%\n",
      "      Val Acc:    88.63%\n",
      "\n",
      "🔄 Epoch 26/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.3830, Acc = 89.1%\n",
      "     Batch  50: Loss = 0.4687, Acc = 88.6%\n",
      "     Batch 100: Loss = 0.3878, Acc = 88.8%\n",
      "     Batch 150: Loss = 0.4815, Acc = 88.6%\n",
      "     Batch 200: Loss = 0.3364, Acc = 88.6%\n",
      "     Batch 250: Loss = 0.5089, Acc = 88.7%\n",
      "     Batch 300: Loss = 0.3856, Acc = 88.8%\n",
      "     Batch 350: Loss = 0.2721, Acc = 88.8%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3783\n",
      "      Train Acc:  88.79%\n",
      "      Val Acc:    88.81%\n",
      "\n",
      "🔄 Epoch 27/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.3938, Acc = 89.8%\n",
      "     Batch  50: Loss = 0.3017, Acc = 88.8%\n",
      "     Batch 100: Loss = 0.3157, Acc = 89.0%\n",
      "     Batch 150: Loss = 0.4348, Acc = 89.1%\n",
      "     Batch 200: Loss = 0.2883, Acc = 88.8%\n",
      "     Batch 250: Loss = 0.3425, Acc = 89.0%\n",
      "     Batch 300: Loss = 0.4980, Acc = 89.0%\n",
      "     Batch 350: Loss = 0.4053, Acc = 89.0%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3721\n",
      "      Train Acc:  88.98%\n",
      "      Val Acc:    89.00%\n",
      "\n",
      "🔄 Epoch 28/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2915, Acc = 93.8%\n",
      "     Batch  50: Loss = 0.4091, Acc = 89.7%\n",
      "     Batch 100: Loss = 0.2237, Acc = 89.1%\n",
      "     Batch 150: Loss = 0.3774, Acc = 89.3%\n",
      "     Batch 200: Loss = 0.4758, Acc = 89.3%\n",
      "     Batch 250: Loss = 0.2817, Acc = 89.3%\n",
      "     Batch 300: Loss = 0.3329, Acc = 89.2%\n",
      "     Batch 350: Loss = 0.4426, Acc = 89.1%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3657\n",
      "      Train Acc:  89.19%\n",
      "      Val Acc:    89.17%\n",
      "\n",
      "🔄 Epoch 29/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4596, Acc = 86.7%\n",
      "     Batch  50: Loss = 0.3024, Acc = 89.7%\n",
      "     Batch 100: Loss = 0.3473, Acc = 89.8%\n",
      "     Batch 150: Loss = 0.4681, Acc = 89.8%\n",
      "     Batch 200: Loss = 0.4542, Acc = 89.6%\n",
      "     Batch 250: Loss = 0.2710, Acc = 89.6%\n",
      "     Batch 300: Loss = 0.3934, Acc = 89.5%\n",
      "     Batch 350: Loss = 0.6068, Acc = 89.6%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3598\n",
      "      Train Acc:  89.49%\n",
      "      Val Acc:    89.21%\n",
      "\n",
      "🔄 Epoch 30/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2779, Acc = 90.6%\n",
      "     Batch  50: Loss = 0.2873, Acc = 89.3%\n",
      "     Batch 100: Loss = 0.5176, Acc = 89.6%\n",
      "     Batch 150: Loss = 0.3407, Acc = 89.5%\n",
      "     Batch 200: Loss = 0.3233, Acc = 89.5%\n",
      "     Batch 250: Loss = 0.2321, Acc = 89.6%\n",
      "     Batch 300: Loss = 0.4877, Acc = 89.6%\n",
      "     Batch 350: Loss = 0.2942, Acc = 89.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3544\n",
      "      Train Acc:  89.56%\n",
      "      Val Acc:    89.44%\n",
      "\n",
      "🔄 Epoch 31/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.5246, Acc = 85.2%\n",
      "     Batch  50: Loss = 0.4245, Acc = 90.1%\n",
      "     Batch 100: Loss = 0.3781, Acc = 89.8%\n",
      "     Batch 150: Loss = 0.4273, Acc = 89.7%\n",
      "     Batch 200: Loss = 0.4968, Acc = 89.7%\n",
      "     Batch 250: Loss = 0.3371, Acc = 89.7%\n",
      "     Batch 300: Loss = 0.2736, Acc = 89.7%\n",
      "     Batch 350: Loss = 0.2467, Acc = 89.7%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3490\n",
      "      Train Acc:  89.70%\n",
      "      Val Acc:    89.53%\n",
      "\n",
      "🔄 Epoch 32/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4482, Acc = 89.8%\n",
      "     Batch  50: Loss = 0.2213, Acc = 89.6%\n",
      "     Batch 100: Loss = 0.3639, Acc = 89.6%\n",
      "     Batch 150: Loss = 0.3010, Acc = 89.8%\n",
      "     Batch 200: Loss = 0.4050, Acc = 89.7%\n",
      "     Batch 250: Loss = 0.3422, Acc = 89.7%\n",
      "     Batch 300: Loss = 0.3810, Acc = 89.7%\n",
      "     Batch 350: Loss = 0.3391, Acc = 89.8%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3438\n",
      "      Train Acc:  89.83%\n",
      "      Val Acc:    89.80%\n",
      "\n",
      "🔄 Epoch 33/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4363, Acc = 87.5%\n",
      "     Batch  50: Loss = 0.3328, Acc = 90.1%\n",
      "     Batch 100: Loss = 0.3594, Acc = 90.1%\n",
      "     Batch 150: Loss = 0.3223, Acc = 90.1%\n",
      "     Batch 200: Loss = 0.2840, Acc = 90.0%\n",
      "     Batch 250: Loss = 0.3548, Acc = 90.1%\n",
      "     Batch 300: Loss = 0.2111, Acc = 90.1%\n",
      "     Batch 350: Loss = 0.3149, Acc = 90.1%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3387\n",
      "      Train Acc:  90.10%\n",
      "      Val Acc:    89.83%\n",
      "\n",
      "🔄 Epoch 34/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4403, Acc = 88.3%\n",
      "     Batch  50: Loss = 0.4598, Acc = 90.3%\n",
      "     Batch 100: Loss = 0.3283, Acc = 90.3%\n",
      "     Batch 150: Loss = 0.5872, Acc = 90.3%\n",
      "     Batch 200: Loss = 0.3418, Acc = 90.2%\n",
      "     Batch 250: Loss = 0.2892, Acc = 90.3%\n",
      "     Batch 300: Loss = 0.3048, Acc = 90.2%\n",
      "     Batch 350: Loss = 0.3950, Acc = 90.3%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3339\n",
      "      Train Acc:  90.24%\n",
      "      Val Acc:    90.19%\n",
      "\n",
      "🔄 Epoch 35/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2630, Acc = 89.1%\n",
      "     Batch  50: Loss = 0.4123, Acc = 90.6%\n",
      "     Batch 100: Loss = 0.2162, Acc = 90.6%\n",
      "     Batch 150: Loss = 0.2334, Acc = 90.5%\n",
      "     Batch 200: Loss = 0.2333, Acc = 90.5%\n",
      "     Batch 250: Loss = 0.3077, Acc = 90.4%\n",
      "     Batch 300: Loss = 0.2098, Acc = 90.3%\n",
      "     Batch 350: Loss = 0.1672, Acc = 90.4%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3288\n",
      "      Train Acc:  90.39%\n",
      "      Val Acc:    90.18%\n",
      "\n",
      "🔄 Epoch 36/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2204, Acc = 92.2%\n",
      "     Batch  50: Loss = 0.3747, Acc = 90.8%\n",
      "     Batch 100: Loss = 0.2902, Acc = 90.7%\n",
      "     Batch 150: Loss = 0.2891, Acc = 90.4%\n",
      "     Batch 200: Loss = 0.2401, Acc = 90.4%\n",
      "     Batch 250: Loss = 0.2312, Acc = 90.5%\n",
      "     Batch 300: Loss = 0.4054, Acc = 90.5%\n",
      "     Batch 350: Loss = 0.3214, Acc = 90.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3241\n",
      "      Train Acc:  90.51%\n",
      "      Val Acc:    90.39%\n",
      "\n",
      "🔄 Epoch 37/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4179, Acc = 86.7%\n",
      "     Batch  50: Loss = 0.2815, Acc = 90.5%\n",
      "     Batch 100: Loss = 0.2807, Acc = 90.8%\n",
      "     Batch 150: Loss = 0.2720, Acc = 90.5%\n",
      "     Batch 200: Loss = 0.2240, Acc = 90.6%\n",
      "     Batch 250: Loss = 0.4287, Acc = 90.5%\n",
      "     Batch 300: Loss = 0.1972, Acc = 90.6%\n",
      "     Batch 350: Loss = 0.3511, Acc = 90.6%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3201\n",
      "      Train Acc:  90.56%\n",
      "      Val Acc:    90.40%\n",
      "\n",
      "🔄 Epoch 38/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2231, Acc = 94.5%\n",
      "     Batch  50: Loss = 0.2599, Acc = 91.3%\n",
      "     Batch 100: Loss = 0.3321, Acc = 91.0%\n",
      "     Batch 150: Loss = 0.3418, Acc = 90.8%\n",
      "     Batch 200: Loss = 0.3757, Acc = 90.9%\n",
      "     Batch 250: Loss = 0.3817, Acc = 90.9%\n",
      "     Batch 300: Loss = 0.2779, Acc = 91.0%\n",
      "     Batch 350: Loss = 0.3327, Acc = 90.8%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3157\n",
      "      Train Acc:  90.70%\n",
      "      Val Acc:    90.46%\n",
      "\n",
      "🔄 Epoch 39/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.3885, Acc = 87.5%\n",
      "     Batch  50: Loss = 0.1862, Acc = 90.7%\n",
      "     Batch 100: Loss = 0.3782, Acc = 90.7%\n",
      "     Batch 150: Loss = 0.3168, Acc = 90.8%\n",
      "     Batch 200: Loss = 0.4825, Acc = 90.8%\n",
      "     Batch 250: Loss = 0.2743, Acc = 90.9%\n",
      "     Batch 300: Loss = 0.3204, Acc = 90.9%\n",
      "     Batch 350: Loss = 0.3374, Acc = 90.9%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3121\n",
      "      Train Acc:  90.83%\n",
      "      Val Acc:    90.53%\n",
      "\n",
      "🔄 Epoch 40/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.3317, Acc = 89.8%\n",
      "     Batch  50: Loss = 0.3732, Acc = 90.9%\n",
      "     Batch 100: Loss = 0.2278, Acc = 90.7%\n",
      "     Batch 150: Loss = 0.2982, Acc = 90.7%\n",
      "     Batch 200: Loss = 0.3011, Acc = 90.8%\n",
      "     Batch 250: Loss = 0.3015, Acc = 90.9%\n",
      "     Batch 300: Loss = 0.2962, Acc = 90.9%\n",
      "     Batch 350: Loss = 0.3408, Acc = 90.9%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3077\n",
      "      Train Acc:  90.96%\n",
      "      Val Acc:    90.83%\n",
      "\n",
      "🔄 Epoch 41/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4488, Acc = 89.8%\n",
      "     Batch  50: Loss = 0.3759, Acc = 91.0%\n",
      "     Batch 100: Loss = 0.3553, Acc = 91.2%\n",
      "     Batch 150: Loss = 0.3077, Acc = 91.2%\n",
      "     Batch 200: Loss = 0.2284, Acc = 91.1%\n",
      "     Batch 250: Loss = 0.2430, Acc = 91.0%\n",
      "     Batch 300: Loss = 0.3280, Acc = 91.1%\n",
      "     Batch 350: Loss = 0.2665, Acc = 91.1%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3034\n",
      "      Train Acc:  91.12%\n",
      "      Val Acc:    90.84%\n",
      "\n",
      "🔄 Epoch 42/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.3964, Acc = 88.3%\n",
      "     Batch  50: Loss = 0.1953, Acc = 91.0%\n",
      "     Batch 100: Loss = 0.2729, Acc = 91.2%\n",
      "     Batch 150: Loss = 0.5954, Acc = 91.2%\n",
      "     Batch 200: Loss = 0.3026, Acc = 91.3%\n",
      "     Batch 250: Loss = 0.3133, Acc = 91.2%\n",
      "     Batch 300: Loss = 0.3805, Acc = 91.2%\n",
      "     Batch 350: Loss = 0.3032, Acc = 91.2%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3000\n",
      "      Train Acc:  91.19%\n",
      "      Val Acc:    90.77%\n",
      "\n",
      "🔄 Epoch 43/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2812, Acc = 93.0%\n",
      "     Batch  50: Loss = 0.2298, Acc = 91.2%\n",
      "     Batch 100: Loss = 0.3396, Acc = 91.4%\n",
      "     Batch 150: Loss = 0.2619, Acc = 91.4%\n",
      "     Batch 200: Loss = 0.3834, Acc = 91.4%\n",
      "     Batch 250: Loss = 0.3217, Acc = 91.4%\n",
      "     Batch 300: Loss = 0.3146, Acc = 91.3%\n",
      "     Batch 350: Loss = 0.2045, Acc = 91.3%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2963\n",
      "      Train Acc:  91.30%\n",
      "      Val Acc:    91.15%\n",
      "\n",
      "🔄 Epoch 44/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2794, Acc = 89.8%\n",
      "     Batch  50: Loss = 0.2601, Acc = 91.3%\n",
      "     Batch 100: Loss = 0.2615, Acc = 91.3%\n",
      "     Batch 150: Loss = 0.2699, Acc = 91.3%\n",
      "     Batch 200: Loss = 0.4255, Acc = 91.4%\n",
      "     Batch 250: Loss = 0.2885, Acc = 91.4%\n",
      "     Batch 300: Loss = 0.2395, Acc = 91.4%\n",
      "     Batch 350: Loss = 0.2535, Acc = 91.4%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2926\n",
      "      Train Acc:  91.41%\n",
      "      Val Acc:    91.06%\n",
      "\n",
      "🔄 Epoch 45/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2853, Acc = 89.1%\n",
      "     Batch  50: Loss = 0.2505, Acc = 91.2%\n",
      "     Batch 100: Loss = 0.3614, Acc = 91.5%\n",
      "     Batch 150: Loss = 0.3325, Acc = 91.6%\n",
      "     Batch 200: Loss = 0.2024, Acc = 91.6%\n",
      "     Batch 250: Loss = 0.3061, Acc = 91.5%\n",
      "     Batch 300: Loss = 0.3224, Acc = 91.5%\n",
      "     Batch 350: Loss = 0.3406, Acc = 91.6%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2890\n",
      "      Train Acc:  91.54%\n",
      "      Val Acc:    91.32%\n",
      "\n",
      "🔄 Epoch 46/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2948, Acc = 90.6%\n",
      "     Batch  50: Loss = 0.2901, Acc = 91.8%\n",
      "     Batch 100: Loss = 0.3924, Acc = 92.0%\n",
      "     Batch 150: Loss = 0.2744, Acc = 91.9%\n",
      "     Batch 200: Loss = 0.2458, Acc = 91.7%\n",
      "     Batch 250: Loss = 0.2474, Acc = 91.6%\n",
      "     Batch 300: Loss = 0.2234, Acc = 91.6%\n",
      "     Batch 350: Loss = 0.2333, Acc = 91.6%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2856\n",
      "      Train Acc:  91.61%\n",
      "      Val Acc:    91.36%\n",
      "\n",
      "🔄 Epoch 47/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2859, Acc = 93.0%\n",
      "     Batch  50: Loss = 0.2892, Acc = 91.5%\n",
      "     Batch 100: Loss = 0.1975, Acc = 91.7%\n",
      "     Batch 150: Loss = 0.3181, Acc = 91.6%\n",
      "     Batch 200: Loss = 0.2488, Acc = 91.6%\n",
      "     Batch 250: Loss = 0.4892, Acc = 91.7%\n",
      "     Batch 300: Loss = 0.1866, Acc = 91.7%\n",
      "     Batch 350: Loss = 0.2209, Acc = 91.7%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2821\n",
      "      Train Acc:  91.75%\n",
      "      Val Acc:    91.50%\n",
      "\n",
      "🔄 Epoch 48/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2801, Acc = 91.4%\n",
      "     Batch  50: Loss = 0.3793, Acc = 91.3%\n",
      "     Batch 100: Loss = 0.3091, Acc = 91.4%\n",
      "     Batch 150: Loss = 0.2818, Acc = 91.3%\n",
      "     Batch 200: Loss = 0.3100, Acc = 91.5%\n",
      "     Batch 250: Loss = 0.2724, Acc = 91.7%\n",
      "     Batch 300: Loss = 0.2329, Acc = 91.7%\n",
      "     Batch 350: Loss = 0.4000, Acc = 91.7%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2787\n",
      "      Train Acc:  91.74%\n",
      "      Val Acc:    91.28%\n",
      "\n",
      "🔄 Epoch 49/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.3577, Acc = 91.4%\n",
      "     Batch  50: Loss = 0.3206, Acc = 92.0%\n",
      "     Batch 100: Loss = 0.1741, Acc = 91.9%\n",
      "     Batch 150: Loss = 0.3866, Acc = 91.6%\n",
      "     Batch 200: Loss = 0.1808, Acc = 91.7%\n",
      "     Batch 250: Loss = 0.2533, Acc = 91.8%\n",
      "     Batch 300: Loss = 0.3775, Acc = 91.9%\n",
      "     Batch 350: Loss = 0.2748, Acc = 91.9%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2759\n",
      "      Train Acc:  91.84%\n",
      "      Val Acc:    91.55%\n",
      "\n",
      "🔄 Epoch 50/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2724, Acc = 92.2%\n",
      "     Batch  50: Loss = 0.3496, Acc = 92.0%\n",
      "     Batch 100: Loss = 0.2050, Acc = 91.9%\n",
      "     Batch 150: Loss = 0.2317, Acc = 92.1%\n",
      "     Batch 200: Loss = 0.2841, Acc = 92.0%\n",
      "     Batch 250: Loss = 0.2465, Acc = 92.0%\n",
      "     Batch 300: Loss = 0.3151, Acc = 92.0%\n",
      "     Batch 350: Loss = 0.2842, Acc = 92.1%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2725\n",
      "      Train Acc:  91.99%\n",
      "      Val Acc:    91.67%\n",
      "\n",
      "🔄 Epoch 51/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4338, Acc = 89.1%\n",
      "     Batch  50: Loss = 0.1794, Acc = 92.2%\n",
      "     Batch 100: Loss = 0.2550, Acc = 92.3%\n",
      "     Batch 150: Loss = 0.2097, Acc = 92.2%\n",
      "     Batch 200: Loss = 0.3073, Acc = 92.1%\n",
      "     Batch 250: Loss = 0.3947, Acc = 92.1%\n",
      "     Batch 300: Loss = 0.2147, Acc = 92.1%\n",
      "     Batch 350: Loss = 0.2380, Acc = 92.1%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2700\n",
      "      Train Acc:  92.05%\n",
      "      Val Acc:    91.78%\n",
      "\n",
      "🔄 Epoch 52/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.1108, Acc = 96.1%\n",
      "     Batch  50: Loss = 0.2836, Acc = 92.4%\n",
      "     Batch 100: Loss = 0.1751, Acc = 92.4%\n",
      "     Batch 150: Loss = 0.2239, Acc = 92.5%\n",
      "     Batch 200: Loss = 0.1840, Acc = 92.3%\n",
      "     Batch 250: Loss = 0.2583, Acc = 92.3%\n",
      "     Batch 300: Loss = 0.2473, Acc = 92.2%\n",
      "     Batch 350: Loss = 0.2628, Acc = 92.2%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2670\n",
      "      Train Acc:  92.14%\n",
      "      Val Acc:    91.77%\n",
      "\n",
      "🔄 Epoch 53/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2985, Acc = 93.0%\n",
      "     Batch  50: Loss = 0.3209, Acc = 92.1%\n",
      "     Batch 100: Loss = 0.2131, Acc = 92.2%\n",
      "     Batch 150: Loss = 0.3042, Acc = 92.1%\n",
      "     Batch 200: Loss = 0.2238, Acc = 92.2%\n",
      "     Batch 250: Loss = 0.2539, Acc = 92.2%\n",
      "     Batch 300: Loss = 0.3132, Acc = 92.3%\n",
      "     Batch 350: Loss = 0.3568, Acc = 92.3%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2643\n",
      "      Train Acc:  92.23%\n",
      "      Val Acc:    91.79%\n",
      "\n",
      "🔄 Epoch 54/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.1433, Acc = 96.1%\n",
      "     Batch  50: Loss = 0.3218, Acc = 92.7%\n",
      "     Batch 100: Loss = 0.2257, Acc = 92.4%\n",
      "     Batch 150: Loss = 0.3226, Acc = 92.2%\n",
      "     Batch 200: Loss = 0.3316, Acc = 92.4%\n",
      "     Batch 250: Loss = 0.2310, Acc = 92.5%\n",
      "     Batch 300: Loss = 0.2754, Acc = 92.5%\n",
      "     Batch 350: Loss = 0.2163, Acc = 92.4%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2612\n",
      "      Train Acc:  92.31%\n",
      "      Val Acc:    91.92%\n",
      "\n",
      "🔄 Epoch 55/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.3406, Acc = 89.8%\n",
      "     Batch  50: Loss = 0.2040, Acc = 92.8%\n",
      "     Batch 100: Loss = 0.2514, Acc = 92.5%\n",
      "     Batch 150: Loss = 0.2059, Acc = 92.3%\n",
      "     Batch 200: Loss = 0.4556, Acc = 92.3%\n",
      "     Batch 250: Loss = 0.1662, Acc = 92.3%\n",
      "     Batch 300: Loss = 0.2262, Acc = 92.3%\n",
      "     Batch 350: Loss = 0.2651, Acc = 92.3%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2580\n",
      "      Train Acc:  92.33%\n",
      "      Val Acc:    92.21%\n",
      "\n",
      "🔄 Epoch 56/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.1391, Acc = 94.5%\n",
      "     Batch  50: Loss = 0.2779, Acc = 92.7%\n",
      "     Batch 100: Loss = 0.2551, Acc = 92.8%\n",
      "     Batch 150: Loss = 0.3520, Acc = 92.7%\n",
      "     Batch 200: Loss = 0.1904, Acc = 92.6%\n",
      "     Batch 250: Loss = 0.3571, Acc = 92.5%\n",
      "     Batch 300: Loss = 0.3080, Acc = 92.5%\n",
      "     Batch 350: Loss = 0.1796, Acc = 92.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2557\n",
      "      Train Acc:  92.48%\n",
      "      Val Acc:    92.10%\n",
      "\n",
      "🔄 Epoch 57/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2872, Acc = 89.8%\n",
      "     Batch  50: Loss = 0.1527, Acc = 92.6%\n",
      "     Batch 100: Loss = 0.3089, Acc = 92.6%\n",
      "     Batch 150: Loss = 0.1994, Acc = 92.6%\n",
      "     Batch 200: Loss = 0.2298, Acc = 92.5%\n",
      "     Batch 250: Loss = 0.3061, Acc = 92.5%\n",
      "     Batch 300: Loss = 0.2717, Acc = 92.5%\n",
      "     Batch 350: Loss = 0.3256, Acc = 92.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2536\n",
      "      Train Acc:  92.51%\n",
      "      Val Acc:    92.22%\n",
      "\n",
      "🔄 Epoch 58/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2830, Acc = 93.0%\n",
      "     Batch  50: Loss = 0.2108, Acc = 92.5%\n",
      "     Batch 100: Loss = 0.3673, Acc = 92.6%\n",
      "     Batch 150: Loss = 0.2304, Acc = 92.5%\n",
      "     Batch 200: Loss = 0.1870, Acc = 92.5%\n",
      "     Batch 250: Loss = 0.2372, Acc = 92.6%\n",
      "     Batch 300: Loss = 0.2437, Acc = 92.5%\n",
      "     Batch 350: Loss = 0.1297, Acc = 92.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2509\n",
      "      Train Acc:  92.55%\n",
      "      Val Acc:    92.21%\n",
      "\n",
      "🔄 Epoch 59/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2878, Acc = 92.2%\n",
      "     Batch  50: Loss = 0.1648, Acc = 92.6%\n",
      "     Batch 100: Loss = 0.2483, Acc = 92.8%\n",
      "     Batch 150: Loss = 0.2928, Acc = 92.7%\n",
      "     Batch 200: Loss = 0.2090, Acc = 92.7%\n",
      "     Batch 250: Loss = 0.4072, Acc = 92.7%\n",
      "     Batch 300: Loss = 0.2487, Acc = 92.7%\n",
      "     Batch 350: Loss = 0.2220, Acc = 92.7%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2482\n",
      "      Train Acc:  92.69%\n",
      "      Val Acc:    92.41%\n",
      "\n",
      "🔄 Epoch 60/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.3430, Acc = 90.6%\n",
      "     Batch  50: Loss = 0.2851, Acc = 92.8%\n",
      "     Batch 100: Loss = 0.3088, Acc = 92.7%\n",
      "     Batch 150: Loss = 0.2700, Acc = 92.7%\n",
      "     Batch 200: Loss = 0.2249, Acc = 92.7%\n",
      "     Batch 250: Loss = 0.2727, Acc = 92.7%\n",
      "     Batch 300: Loss = 0.2651, Acc = 92.7%\n",
      "     Batch 350: Loss = 0.2822, Acc = 92.7%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2464\n",
      "      Train Acc:  92.73%\n",
      "      Val Acc:    92.35%\n",
      "\n",
      "🔄 Epoch 61/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.1527, Acc = 95.3%\n",
      "     Batch  50: Loss = 0.3917, Acc = 92.6%\n",
      "     Batch 100: Loss = 0.3441, Acc = 92.9%\n",
      "     Batch 150: Loss = 0.2891, Acc = 92.9%\n",
      "     Batch 200: Loss = 0.1799, Acc = 92.8%\n",
      "     Batch 250: Loss = 0.3237, Acc = 92.8%\n",
      "     Batch 300: Loss = 0.2781, Acc = 92.9%\n",
      "     Batch 350: Loss = 0.2980, Acc = 92.8%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2437\n",
      "      Train Acc:  92.80%\n",
      "      Val Acc:    92.40%\n",
      "\n",
      "🔄 Epoch 62/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.1822, Acc = 94.5%\n",
      "     Batch  50: Loss = 0.2415, Acc = 93.2%\n",
      "     Batch 100: Loss = 0.3872, Acc = 92.8%\n",
      "     Batch 150: Loss = 0.2931, Acc = 92.8%\n",
      "     Batch 200: Loss = 0.1881, Acc = 92.8%\n",
      "     Batch 250: Loss = 0.2407, Acc = 92.9%\n",
      "     Batch 300: Loss = 0.3343, Acc = 92.9%\n",
      "     Batch 350: Loss = 0.1995, Acc = 92.8%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2412\n",
      "      Train Acc:  92.87%\n",
      "      Val Acc:    92.45%\n",
      "\n",
      "🔄 Epoch 63/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.1908, Acc = 92.2%\n",
      "     Batch  50: Loss = 0.3380, Acc = 93.0%\n",
      "     Batch 100: Loss = 0.1860, Acc = 92.7%\n",
      "     Batch 150: Loss = 0.3190, Acc = 93.0%\n",
      "     Batch 200: Loss = 0.4796, Acc = 93.0%\n",
      "     Batch 250: Loss = 0.1749, Acc = 93.0%\n",
      "     Batch 300: Loss = 0.2727, Acc = 93.0%\n",
      "     Batch 350: Loss = 0.1828, Acc = 93.0%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2390\n",
      "      Train Acc:  92.96%\n",
      "      Val Acc:    92.59%\n",
      "\n",
      "🔄 Epoch 64/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.3608, Acc = 89.8%\n",
      "     Batch  50: Loss = 0.1899, Acc = 92.9%\n",
      "     Batch 100: Loss = 0.1821, Acc = 92.8%\n",
      "     Batch 150: Loss = 0.1431, Acc = 92.9%\n",
      "     Batch 200: Loss = 0.1511, Acc = 92.9%\n",
      "     Batch 250: Loss = 0.3196, Acc = 92.9%\n",
      "     Batch 300: Loss = 0.1905, Acc = 93.0%\n",
      "     Batch 350: Loss = 0.1832, Acc = 92.9%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2373\n",
      "      Train Acc:  92.92%\n",
      "      Val Acc:    92.55%\n",
      "\n",
      "🔄 Epoch 65/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2976, Acc = 91.4%\n",
      "     Batch  50: Loss = 0.2215, Acc = 93.1%\n",
      "     Batch 100: Loss = 0.3418, Acc = 93.3%\n",
      "     Batch 150: Loss = 0.2261, Acc = 93.1%\n",
      "     Batch 200: Loss = 0.3724, Acc = 93.2%\n",
      "     Batch 250: Loss = 0.1801, Acc = 93.2%\n",
      "     Batch 300: Loss = 0.2367, Acc = 93.1%\n",
      "     Batch 350: Loss = 0.2375, Acc = 93.1%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2347\n",
      "      Train Acc:  93.06%\n",
      "      Val Acc:    92.62%\n",
      "\n",
      "🔄 Epoch 66/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.1605, Acc = 93.8%\n",
      "     Batch  50: Loss = 0.1976, Acc = 93.3%\n",
      "     Batch 100: Loss = 0.1752, Acc = 93.3%\n",
      "     Batch 150: Loss = 0.2686, Acc = 93.2%\n",
      "     Batch 200: Loss = 0.2251, Acc = 93.1%\n",
      "     Batch 250: Loss = 0.2446, Acc = 93.1%\n",
      "     Batch 300: Loss = 0.1889, Acc = 93.1%\n",
      "     Batch 350: Loss = 0.2186, Acc = 93.0%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2327\n",
      "      Train Acc:  93.08%\n",
      "      Val Acc:    92.69%\n",
      "\n",
      "🔄 Epoch 67/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2883, Acc = 92.2%\n",
      "     Batch  50: Loss = 0.2223, Acc = 93.3%\n",
      "     Batch 100: Loss = 0.2342, Acc = 93.1%\n",
      "     Batch 150: Loss = 0.2755, Acc = 93.0%\n",
      "     Batch 200: Loss = 0.1738, Acc = 93.1%\n",
      "     Batch 250: Loss = 0.2865, Acc = 93.1%\n",
      "     Batch 300: Loss = 0.2810, Acc = 93.1%\n",
      "     Batch 350: Loss = 0.1545, Acc = 93.2%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2311\n",
      "      Train Acc:  93.16%\n",
      "      Val Acc:    92.81%\n",
      "\n",
      "🔄 Epoch 68/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.1701, Acc = 96.1%\n",
      "     Batch  50: Loss = 0.2558, Acc = 93.3%\n",
      "     Batch 100: Loss = 0.1831, Acc = 93.4%\n",
      "     Batch 150: Loss = 0.2591, Acc = 93.4%\n",
      "     Batch 200: Loss = 0.3366, Acc = 93.2%\n",
      "     Batch 250: Loss = 0.1299, Acc = 93.2%\n",
      "     Batch 300: Loss = 0.4102, Acc = 93.3%\n",
      "     Batch 350: Loss = 0.1879, Acc = 93.3%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.2291\n",
      "      Train Acc:  93.27%\n",
      "      Val Acc:    93.02%\n",
      "\n",
      "🔄 Epoch 69/100\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.2785, Acc = 92.2%\n",
      "     Batch  50: Loss = 0.1199, Acc = 93.4%\n",
      "     Batch 100: Loss = 0.3078, Acc = 93.4%\n",
      "     Batch 150: Loss = 0.2037, Acc = 93.5%\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    Simple training loop with detailed explanations\n",
    "    Focus: Understanding each step of the training process\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f\"=== TRAINING SETUP ===\")\n",
    "    print(f\"📚 Epochs: {num_epochs}\")\n",
    "    print(f\"📈 Learning rate: {learning_rate}\")\n",
    "    print(f\"💡 Optimizer: Adam\")\n",
    "    print(f\"🎯 Loss function: CrossEntropyLoss\")\n",
    "    \n",
    "    # Track training progress\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    print(f\"\\n=== STARTING TRAINING ===\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # === TRAINING PHASE ===\n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        num_batches = 0  # Track number of batches manually\n",
    "        \n",
    "        print(f\"\\n🔄 Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"   Training...\")\n",
    "        \n",
    "        for batch_idx, (patches, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            logits = model(patches)  # [batch_size, 10]\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            loss.backward()        # Compute gradients\n",
    "            optimizer.step()       # Update parameters\n",
    "            \n",
    "            # Track statistics\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                current_acc = 100. * correct_predictions / total_samples\n",
    "                print(f\"     Batch {batch_idx:3d}: Loss = {loss.item():.4f}, Acc = {current_acc:.1f}%\")\n",
    "        \n",
    "        # Calculate epoch statistics using batch count instead of len()\n",
    "        avg_train_loss = total_train_loss / num_batches\n",
    "        train_accuracy = 100. * correct_predictions / total_samples\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # === VALIDATION PHASE ===\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        print(\"   Validating...\")\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            for patches, labels in val_loader:\n",
    "                logits = model(patches)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"   📊 Results:\")\n",
    "        print(f\"      Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"      Train Acc:  {train_accuracy:.2f}%\")\n",
    "        print(f\"      Val Acc:    {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Simple early stopping if validation accuracy is very high\n",
    "        if val_accuracy > 95.0:\n",
    "            print(f\"   🎉 Great accuracy achieved! Stopping early.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n=== TRAINING COMPLETED ===\")\n",
    "    print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "\n",
    "# First, let's create proper train and validation loaders\n",
    "train_patch_loader = SquareImageSplitingLoader(train_loader)\n",
    "val_patch_loader = SquareImageSplitingLoader(validation_loader)\n",
    "\n",
    "print(\"=== DATA SETUP ===\")\n",
    "print(f\"📚 Training data ready\")\n",
    "print(f\"📖 Validation data ready\")\n",
    "\n",
    "# Let's train our model!\n",
    "print(\"\\nStarting training process...\")\n",
    "history = train_model(model, train_patch_loader, val_patch_loader, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's fix the validation loader issue and create a proper validation set\n",
    "from loaders import MnistLoader, SquareImageSplitingLoader\n",
    "\n",
    "# Reload with proper validation\n",
    "mnist_loader = MnistLoader()\n",
    "train_loader, validation_loader = mnist_loader.get_loaders()\n",
    "\n",
    "# Create patch loaders for both train and validation\n",
    "train_patch_loader = SquareImageSplitingLoader(train_loader)\n",
    "val_patch_loader = SquareImageSplitingLoader(validation_loader)\n",
    "\n",
    "print(\"=== DATA SETUP ===\")\n",
    "print(f\"📚 Training batches: {len(train_loader)}\")\n",
    "print(f\"📖 Validation batches: {len(validation_loader)}\")\n",
    "\n",
    "# Now let's train with proper validation\n",
    "print(\"=== RETRAINING WITH PROPER VALIDATION ===\")\n",
    "model_v2 = SimpleVisionTransformer()  # Fresh model\n",
    "history = train_model(model_v2, train_patch_loader, val_patch_loader, num_epochs=100, learning_rate=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_model_understanding(model, val_loader):\n",
    "    \"\"\"\n",
    "    Analyze what the model learned - for educational purposes\n",
    "    \"\"\"\n",
    "    print(\"=== MODEL ANALYSIS ===\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Test on a few examples\n",
    "    correct_predictions = []\n",
    "    wrong_predictions = []\n",
    "    attention_patterns = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (patches, labels) in enumerate(val_loader):\n",
    "            if batch_idx > 5:  # Just analyze first few batches\n",
    "                break\n",
    "                \n",
    "            # Get model predictions\n",
    "            logits = model(patches)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            \n",
    "            # Store examples\n",
    "            for i in range(min(5, patches.size(0))):  # First 5 images in batch\n",
    "                actual = labels[i].item()\n",
    "                pred = predicted[i].item()\n",
    "                \n",
    "                if actual == pred:\n",
    "                    correct_predictions.append((patches[i], actual, pred))\n",
    "                else:\n",
    "                    wrong_predictions.append((patches[i], actual, pred))\n",
    "                \n",
    "                if len(correct_predictions) >= 5 and len(wrong_predictions) >= 3:\n",
    "                    break\n",
    "            \n",
    "            if len(correct_predictions) >= 5 and len(wrong_predictions) >= 3:\n",
    "                break\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"✅ Found {len(correct_predictions)} correct predictions\")\n",
    "    print(f\"❌ Found {len(wrong_predictions)} wrong predictions\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\n📊 CORRECT PREDICTIONS:\")\n",
    "    for i, (patches, actual, pred) in enumerate(correct_predictions[:3]):\n",
    "        print(f\"   Example {i+1}: Actual = {actual}, Predicted = {pred} ✓\")\n",
    "    \n",
    "    print(\"\\n📊 WRONG PREDICTIONS:\")\n",
    "    for i, (patches, actual, pred) in enumerate(wrong_predictions[:3]):\n",
    "        print(f\"   Example {i+1}: Actual = {actual}, Predicted = {pred} ✗\")\n",
    "    \n",
    "    return correct_predictions, wrong_predictions\n",
    "\n",
    "def visualize_training_progress(history):\n",
    "    \"\"\"\n",
    "    Simple visualization of training progress\n",
    "    \"\"\"\n",
    "    print(\"\\n=== TRAINING PROGRESS ===\")\n",
    "    \n",
    "    epochs = range(1, len(history['train_losses']) + 1)\n",
    "    \n",
    "    print(\"📈 Training Progress:\")\n",
    "    for i, epoch in enumerate(epochs):\n",
    "        print(f\"   Epoch {epoch}:\")\n",
    "        print(f\"      Loss: {history['train_losses'][i]:.4f}\")\n",
    "        print(f\"      Train Acc: {history['train_accuracies'][i]:.2f}%\")\n",
    "        print(f\"      Val Acc: {history['val_accuracies'][i]:.2f}%\")\n",
    "    \n",
    "    # Show improvement\n",
    "    initial_val_acc = history['val_accuracies'][0]\n",
    "    final_val_acc = history['val_accuracies'][-1]\n",
    "    improvement = final_val_acc - initial_val_acc\n",
    "    \n",
    "    print(f\"\\n🎯 OVERALL IMPROVEMENT:\")\n",
    "    print(f\"   Initial validation accuracy: {initial_val_acc:.2f}%\")\n",
    "    print(f\"   Final validation accuracy: {final_val_acc:.2f}%\")\n",
    "    print(f\"   Improvement: +{improvement:.2f}%\")\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "# Analyze our trained model\n",
    "correct_examples, wrong_examples = analyze_model_understanding(model_v2, val_patch_loader)\n",
    "epochs = visualize_training_progress(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
