{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist dataset loaded, train data size: 50000 validation data size: 10000\n"
     ]
    }
   ],
   "source": [
    "from loaders import MnistLoader, SquareImageSplitingLoader\n",
    "\n",
    "mnist_loader = MnistLoader()\n",
    "\n",
    "train_loader, validation_loader = mnist_loader.get_loaders()\n",
    "\n",
    "square_image_spliting_loader = SquareImageSplitingLoader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING SIMPLE VISION TRANSFORMER ===\n",
      "📦 Patch embedding: 49 -> 32\n",
      "📍 Positional embedding: torch.Size([1, 16, 32])\n",
      "🔍 Self-attention matrices: 3 x (32 -> 32)\n",
      "🔄 FFN: 32 -> 64 -> 32\n",
      "🎯 Classifier: 32 -> 10\n",
      "\n",
      "📊 Model Statistics:\n",
      "   Total parameters: 9,930\n",
      "   Trainable parameters: 9,930\n",
      "\n",
      "🧪 Testing forward pass...\n",
      "Input shape: torch.Size([128, 16, 1, 7, 7])\n",
      "Labels shape: torch.Size([128])\n",
      "torch.Size([128, 16, 32])\n",
      "Output logits: torch.Size([128, 10])\n",
      "Sample predictions: tensor([-0.2138, -0.6572, -0.0332, -0.2498,  0.4151, -0.0088, -0.1552, -0.0837,\n",
      "        -0.1780,  0.0376], grad_fn=<SelectBackward0>)\n",
      "\n",
      "✅ Model ready for training!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SimpleVisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Vision Transformer for MNIST classification\n",
    "    Focus: Understanding the basics step by step\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_dim=49, embed_dim=32, num_patches=16, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Step 1: Patch Embedding\n",
    "        self.patch_embedding = nn.Linear(patch_dim, embed_dim)\n",
    "        print(f\"📦 Patch embedding: {patch_dim} -> {embed_dim}\")\n",
    "        \n",
    "        # Step 2: Positional Encoding (learnable - simpler than sinusoidal)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        print(f\"📍 Positional embedding: {self.pos_embedding.shape}\")\n",
    "        \n",
    "        # Step 3: Self-Attention (single head)\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        print(f\"🔍 Self-attention matrices: 3 x ({embed_dim} -> {embed_dim})\")\n",
    "        \n",
    "        # Step 4: Feed Forward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),  # Expand\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),  # Contract\n",
    "        )\n",
    "        print(f\"🔄 FFN: {embed_dim} -> {embed_dim * 2} -> {embed_dim}\")\n",
    "        \n",
    "        # Step 5: Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Step 6: Classification Head\n",
    "        # We'll use global average pooling across patches\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "        print(f\"🎯 Classifier: {embed_dim} -> {num_classes}\")\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with detailed comments for learning\n",
    "        Input: x shape [batch_size, 16, 1, 7, 7] - 16 patches per image\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Step 1: Flatten patches and embed them\n",
    "        x = x.flatten(start_dim=2)  # [batch_size, 16, 49]\n",
    "        x = self.patch_embedding(x)  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 2: Add positional encoding\n",
    "        x = x + self.pos_embedding  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 3: Self-Attention Block\n",
    "        # Save input for residual connection\n",
    "        residual = x\n",
    "        \n",
    "        # Apply layer norm BEFORE attention (Pre-LN architecture)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = self.W_q(x)  # [batch_size, 16, 32]\n",
    "        K = self.W_k(x)  # [batch_size, 16, 32] \n",
    "        V = self.W_v(x)  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_scores = Q @ K.transpose(-2, -1) / (self.embed_dim ** 0.5)  # [batch_size, 16, 16]\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attended_values = attention_weights @ V  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Add residual connection\n",
    "        x = residual + attended_values  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 4: Feed Forward Block\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + x  # [batch_size, 16, 32]\n",
    "        \n",
    "        # Step 5: Classification\n",
    "        # Global average pooling across patches\n",
    "        x = x.mean(dim=1)  # [batch_size, 32] - average across all patches\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(x)  # [batch_size, 10]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create the model\n",
    "print(\"=== CREATING SIMPLE VISION TRANSFORMER ===\")\n",
    "model = SimpleVisionTransformer()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n📊 Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\n🧪 Testing forward pass...\")\n",
    "for patches, labels in square_image_spliting_loader:\n",
    "    print(f\"Input shape: {patches.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    logits = model(patches)\n",
    "    print(f\"Output logits: {logits.shape}\")\n",
    "    print(f\"Sample predictions: {logits[0]}\")\n",
    "    \n",
    "    break\n",
    "\n",
    "print(\"\\n✅ Model ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SETUP ===\n",
      "📚 Training data ready\n",
      "📖 Validation data ready\n",
      "\n",
      "Starting training process...\n",
      "=== TRAINING SETUP ===\n",
      "📚 Epochs: 3\n",
      "📈 Learning rate: 0.001\n",
      "💡 Optimizer: Adam\n",
      "🎯 Loss function: CrossEntropyLoss\n",
      "\n",
      "=== STARTING TRAINING ===\n",
      "\n",
      "🔄 Epoch 1/3\n",
      "   Training...\n",
      "     Batch   0: Loss = 2.3191, Acc = 15.6%\n",
      "     Batch  50: Loss = 2.2037, Acc = 20.4%\n",
      "     Batch 100: Loss = 1.4887, Acc = 28.1%\n",
      "     Batch 150: Loss = 1.0586, Acc = 36.6%\n",
      "     Batch 200: Loss = 1.0520, Acc = 43.4%\n",
      "     Batch 250: Loss = 0.9778, Acc = 48.3%\n",
      "     Batch 300: Loss = 0.7408, Acc = 52.0%\n",
      "     Batch 350: Loss = 0.8717, Acc = 55.0%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 1.2858\n",
      "      Train Acc:  56.90%\n",
      "      Val Acc:    76.44%\n",
      "\n",
      "🔄 Epoch 2/3\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.8328, Acc = 71.1%\n",
      "     Batch  50: Loss = 0.6622, Acc = 77.4%\n",
      "     Batch 100: Loss = 0.4556, Acc = 78.4%\n",
      "     Batch 150: Loss = 0.7794, Acc = 78.6%\n",
      "     Batch 200: Loss = 0.6923, Acc = 79.0%\n",
      "     Batch 250: Loss = 0.4897, Acc = 79.5%\n",
      "     Batch 300: Loss = 0.5545, Acc = 80.0%\n",
      "     Batch 350: Loss = 0.4640, Acc = 80.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.6158\n",
      "      Train Acc:  80.91%\n",
      "      Val Acc:    84.01%\n",
      "\n",
      "🔄 Epoch 3/3\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.4445, Acc = 85.9%\n",
      "     Batch  50: Loss = 0.4266, Acc = 83.8%\n",
      "     Batch 100: Loss = 0.4336, Acc = 84.4%\n",
      "     Batch 150: Loss = 0.3958, Acc = 84.8%\n",
      "     Batch 200: Loss = 0.3679, Acc = 84.9%\n",
      "     Batch 250: Loss = 0.4873, Acc = 85.2%\n",
      "     Batch 300: Loss = 0.3227, Acc = 85.5%\n",
      "     Batch 350: Loss = 0.5137, Acc = 85.7%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.4616\n",
      "      Train Acc:  85.91%\n",
      "      Val Acc:    87.63%\n",
      "\n",
      "=== TRAINING COMPLETED ===\n",
      "Final validation accuracy: 87.63%\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Simple training loop with detailed explanations\n",
    "    Focus: Understanding each step of the training process\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(f\"=== TRAINING SETUP ===\")\n",
    "    print(f\"📚 Epochs: {num_epochs}\")\n",
    "    print(f\"📈 Learning rate: {learning_rate}\")\n",
    "    print(f\"💡 Optimizer: Adam\")\n",
    "    print(f\"🎯 Loss function: CrossEntropyLoss\")\n",
    "    \n",
    "    # Track training progress\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    print(f\"\\n=== STARTING TRAINING ===\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # === TRAINING PHASE ===\n",
    "        model.train()  # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        num_batches = 0  # Track number of batches manually\n",
    "        \n",
    "        print(f\"\\n🔄 Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"   Training...\")\n",
    "        \n",
    "        for batch_idx, (patches, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            logits = model(patches)  # [batch_size, 10]\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            loss.backward()        # Compute gradients\n",
    "            optimizer.step()       # Update parameters\n",
    "            \n",
    "            # Track statistics\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                current_acc = 100. * correct_predictions / total_samples\n",
    "                print(f\"     Batch {batch_idx:3d}: Loss = {loss.item():.4f}, Acc = {current_acc:.1f}%\")\n",
    "        \n",
    "        # Calculate epoch statistics using batch count instead of len()\n",
    "        avg_train_loss = total_train_loss / num_batches\n",
    "        train_accuracy = 100. * correct_predictions / total_samples\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # === VALIDATION PHASE ===\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        print(\"   Validating...\")\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            for patches, labels in val_loader:\n",
    "                logits = model(patches)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"   📊 Results:\")\n",
    "        print(f\"      Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"      Train Acc:  {train_accuracy:.2f}%\")\n",
    "        print(f\"      Val Acc:    {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Simple early stopping if validation accuracy is very high\n",
    "        if val_accuracy > 95.0:\n",
    "            print(f\"   🎉 Great accuracy achieved! Stopping early.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n=== TRAINING COMPLETED ===\")\n",
    "    print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "\n",
    "# First, let's create proper train and validation loaders\n",
    "train_patch_loader = SquareImageSplitingLoader(train_loader)\n",
    "val_patch_loader = SquareImageSplitingLoader(validation_loader)\n",
    "\n",
    "print(\"=== DATA SETUP ===\")\n",
    "print(f\"📚 Training data ready\")\n",
    "print(f\"📖 Validation data ready\")\n",
    "\n",
    "# Let's train our model!\n",
    "print(\"\\nStarting training process...\")\n",
    "history = train_model(model, train_patch_loader, val_patch_loader, num_epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist dataset loaded, train data size: 50000 validation data size: 10000\n",
      "=== DATA SETUP ===\n",
      "📚 Training batches: 391\n",
      "📖 Validation batches: 79\n",
      "=== RETRAINING WITH PROPER VALIDATION ===\n",
      "📦 Patch embedding: 49 -> 32\n",
      "📍 Positional embedding: torch.Size([1, 16, 32])\n",
      "🔍 Self-attention matrices: 3 x (32 -> 32)\n",
      "🔄 FFN: 32 -> 64 -> 32\n",
      "🎯 Classifier: 32 -> 10\n",
      "=== TRAINING SETUP ===\n",
      "📚 Epochs: 3\n",
      "📈 Learning rate: 0.001\n",
      "💡 Optimizer: Adam\n",
      "🎯 Loss function: CrossEntropyLoss\n",
      "\n",
      "=== STARTING TRAINING ===\n",
      "\n",
      "🔄 Epoch 1/3\n",
      "   Training...\n",
      "     Batch   0: Loss = 2.3452, Acc = 9.4%\n",
      "     Batch  50: Loss = 2.2012, Acc = 20.0%\n",
      "     Batch 100: Loss = 1.5574, Acc = 28.0%\n",
      "     Batch 150: Loss = 1.1949, Acc = 37.6%\n",
      "     Batch 200: Loss = 0.7770, Acc = 45.6%\n",
      "     Batch 250: Loss = 1.0475, Acc = 51.3%\n",
      "     Batch 300: Loss = 0.7425, Acc = 55.5%\n",
      "     Batch 350: Loss = 0.5481, Acc = 58.7%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 1.1855\n",
      "      Train Acc:  60.85%\n",
      "      Val Acc:    79.73%\n",
      "\n",
      "🔄 Epoch 2/3\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.5575, Acc = 80.5%\n",
      "     Batch  50: Loss = 0.6193, Acc = 80.9%\n",
      "     Batch 100: Loss = 0.6945, Acc = 81.1%\n",
      "     Batch 150: Loss = 0.6738, Acc = 81.4%\n",
      "     Batch 200: Loss = 0.5673, Acc = 82.0%\n",
      "     Batch 250: Loss = 0.5522, Acc = 82.6%\n",
      "     Batch 300: Loss = 0.4221, Acc = 83.2%\n",
      "     Batch 350: Loss = 0.4655, Acc = 83.5%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.5217\n",
      "      Train Acc:  83.81%\n",
      "      Val Acc:    86.08%\n",
      "\n",
      "🔄 Epoch 3/3\n",
      "   Training...\n",
      "     Batch   0: Loss = 0.5124, Acc = 86.7%\n",
      "     Batch  50: Loss = 0.3535, Acc = 87.2%\n",
      "     Batch 100: Loss = 0.4035, Acc = 87.0%\n",
      "     Batch 150: Loss = 0.4060, Acc = 87.3%\n",
      "     Batch 200: Loss = 0.4514, Acc = 87.7%\n",
      "     Batch 250: Loss = 0.4532, Acc = 87.8%\n",
      "     Batch 300: Loss = 0.4279, Acc = 88.0%\n",
      "     Batch 350: Loss = 0.4111, Acc = 88.0%\n",
      "   Validating...\n",
      "   📊 Results:\n",
      "      Train Loss: 0.3921\n",
      "      Train Acc:  88.17%\n",
      "      Val Acc:    89.27%\n",
      "\n",
      "=== TRAINING COMPLETED ===\n",
      "Final validation accuracy: 89.27%\n"
     ]
    }
   ],
   "source": [
    "# First, let's fix the validation loader issue and create a proper validation set\n",
    "from loaders import MnistLoader, SquareImageSplitingLoader\n",
    "\n",
    "# Reload with proper validation\n",
    "mnist_loader = MnistLoader()\n",
    "train_loader, validation_loader = mnist_loader.get_loaders()\n",
    "\n",
    "# Create patch loaders for both train and validation\n",
    "train_patch_loader = SquareImageSplitingLoader(train_loader)\n",
    "val_patch_loader = SquareImageSplitingLoader(validation_loader)\n",
    "\n",
    "print(\"=== DATA SETUP ===\")\n",
    "print(f\"📚 Training batches: {len(train_loader)}\")\n",
    "print(f\"📖 Validation batches: {len(validation_loader)}\")\n",
    "\n",
    "# Now let's train with proper validation\n",
    "print(\"=== RETRAINING WITH PROPER VALIDATION ===\")\n",
    "model_v2 = SimpleVisionTransformer()  # Fresh model\n",
    "history = train_model(model_v2, train_patch_loader, val_patch_loader, num_epochs=3, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyze_model_understanding\u001b[39m(model, val_loader):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def analyze_model_understanding(model, val_loader):\n",
    "    \"\"\"\n",
    "    Analyze what the model learned - for educational purposes\n",
    "    \"\"\"\n",
    "    print(\"=== MODEL ANALYSIS ===\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Test on a few examples\n",
    "    correct_predictions = []\n",
    "    wrong_predictions = []\n",
    "    attention_patterns = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (patches, labels) in enumerate(val_loader):\n",
    "            if batch_idx > 5:  # Just analyze first few batches\n",
    "                break\n",
    "                \n",
    "            # Get model predictions\n",
    "            logits = model(patches)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            \n",
    "            # Store examples\n",
    "            for i in range(min(5, patches.size(0))):  # First 5 images in batch\n",
    "                actual = labels[i].item()\n",
    "                pred = predicted[i].item()\n",
    "                \n",
    "                if actual == pred:\n",
    "                    correct_predictions.append((patches[i], actual, pred))\n",
    "                else:\n",
    "                    wrong_predictions.append((patches[i], actual, pred))\n",
    "                \n",
    "                if len(correct_predictions) >= 5 and len(wrong_predictions) >= 3:\n",
    "                    break\n",
    "            \n",
    "            if len(correct_predictions) >= 5 and len(wrong_predictions) >= 3:\n",
    "                break\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"✅ Found {len(correct_predictions)} correct predictions\")\n",
    "    print(f\"❌ Found {len(wrong_predictions)} wrong predictions\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\n📊 CORRECT PREDICTIONS:\")\n",
    "    for i, (patches, actual, pred) in enumerate(correct_predictions[:3]):\n",
    "        print(f\"   Example {i+1}: Actual = {actual}, Predicted = {pred} ✓\")\n",
    "    \n",
    "    print(\"\\n📊 WRONG PREDICTIONS:\")\n",
    "    for i, (patches, actual, pred) in enumerate(wrong_predictions[:3]):\n",
    "        print(f\"   Example {i+1}: Actual = {actual}, Predicted = {pred} ✗\")\n",
    "    \n",
    "    return correct_predictions, wrong_predictions\n",
    "\n",
    "def visualize_training_progress(history):\n",
    "    \"\"\"\n",
    "    Simple visualization of training progress\n",
    "    \"\"\"\n",
    "    print(\"\\n=== TRAINING PROGRESS ===\")\n",
    "    \n",
    "    epochs = range(1, len(history['train_losses']) + 1)\n",
    "    \n",
    "    print(\"📈 Training Progress:\")\n",
    "    for i, epoch in enumerate(epochs):\n",
    "        print(f\"   Epoch {epoch}:\")\n",
    "        print(f\"      Loss: {history['train_losses'][i]:.4f}\")\n",
    "        print(f\"      Train Acc: {history['train_accuracies'][i]:.2f}%\")\n",
    "        print(f\"      Val Acc: {history['val_accuracies'][i]:.2f}%\")\n",
    "    \n",
    "    # Show improvement\n",
    "    initial_val_acc = history['val_accuracies'][0]\n",
    "    final_val_acc = history['val_accuracies'][-1]\n",
    "    improvement = final_val_acc - initial_val_acc\n",
    "    \n",
    "    print(f\"\\n🎯 OVERALL IMPROVEMENT:\")\n",
    "    print(f\"   Initial validation accuracy: {initial_val_acc:.2f}%\")\n",
    "    print(f\"   Final validation accuracy: {final_val_acc:.2f}%\")\n",
    "    print(f\"   Improvement: +{improvement:.2f}%\")\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "# Analyze our trained model\n",
    "correct_examples, wrong_examples = analyze_model_understanding(model_v2, val_patch_loader)\n",
    "epochs = visualize_training_progress(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
